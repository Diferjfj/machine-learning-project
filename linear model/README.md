# 线性回归模型
## 1.正规方程
直接通过矩阵方程求得最小平方误差的解 $\theta=(X^TX)^{-1} X^T y$<br>
注意矩阵有可能不可逆或者稳定性太差，需要先填充数据中的缺失值，使用伪逆求解。（注：数据缩放对结果没有影响，可能对矩阵的稳定性有一定帮助）
## 2.梯度下降
创建一个LinerRegressionGD类，在类里面实现前向传播，计算梯度，反向传播（实际只有一层，其实无所谓反向传播），更新权重等函数。<br>梯度下降公式；
$\theta =\theta-\eta *\frac{\partial J}{\partial \theta}$<br>
一开始，我没有使用特征缩放，导致学习率必须设置的非常低，最后的预测效果非常不好，损失函数变化很小。这说明在梯度下降算法中，特征缩放是很有必要的，这与正规方程不同。
## 3.加州房价预测
对于这个数据集来说，线性回归模型的表现并不好。但是训练这个模型还是能给我们带来一些启发。<br>
首先，这个数据一共有八个特征，其中ocean_proximity表征的是房子离海岸的远近，数据是字符串，需要进行处理。我采取的是独热编码的模式，把ocean_proximity中每一个字符串看成一个特殊的属性，将这些新属性添加到原数据中，并将原数据所带字符串对应的属性值置1，其余置零。我使用pandas自带的get_dummies函数实现编码。<br>
随后，再对数据中的缺失值进行处理，可以直接删除掉有缺失值的数据，也可以用特定值代替（比如平均值），从逻辑上来说应该删去这些数据效果更好，但是实际结果不管是删去数据还是填充平均值影响均不大，因为线性模型在这里本来就不够精确。<br>
从结果上来看，当使用学习率为0.1，训练循环1000次时，误差已经几乎不再变化，训练得到的结果与正规方程得到的结果非常接近，测试集前五个数据的百分比误差完全一样，这说明两个算法得到的结果是一样的。但是梯度下降必须要进行特征缩放，否则学习率不好调整，甚至会出现预测值是负数的情况。正规方程的代码比较简单，运行速度也比较快，但是不易于拓展，只能处理线性模型，而且不能与其他模型相结合使用。
